sent_text
The paper basicaly propose keep using the typicaldata-augmentation transformations done during training alo in evalation time, to prevent adversarialattacks.
In the paper they analze only 2 random resizing and random padding, but I suppose others like random contrast, random relighting, random colorization, ... could be applicable.
Some of the pros of the proposed tricks is that it doesn't reqire re-training existing models, alhough as the authors pointed out re-training for adversarialimages is necessary to obtain good results.
Typicaly images have different sizes, however in the Dataset are described as having 299x299x3 size, are al the test images resized before hand?
How would this method work with variable size images?
The proposed defense reqires increasing the size of the input images, have you analzed the impact in performance?
Also it would be good to know how robust is the method for smaler sizes.
secion 4 6 2 seems to indicate that 1 pixel padding or just resizing 1 pixel is enough to get most of the benefit, please provide an analsis of how results improve as the padding or size increase.
In secion 5 for the chalenge authors used a lot more evalations per image, could you provide how much extra computation is needed for that model?
The authors consider new attacks for generating adversarialsamples against neuralnetworks.
In particular, they are interested in approximating gradient-based white-box attacks such as FGSM in a black-box setting by estimating gradients from queries to the classifier.
They assume that the attacker is able to query, for any example x, the vector of probabilities p(x) corresponding to each class.
Given such query access it’s trivialto estimate the gradients of p using finite differences.
As a conseqence one can implement FGSM using these estimates assuming cross-entropy loss, as well as a logit-based loss.
They consider both iterative and single-step FGSM attacks in the targeted (i e the adversary’s goalis to switch the example’s label to a specific alernative label) and un-targeted settings (any mislabelling is a success).
They compare themselves to transfer black-box attacks, where the adversary trains a proxy model and generates the adversarialsample by running a white-box attack on that model.
For a number of target classifiers on both MNIST and CIFAR-10, they show that these attacks outperform the transfer-based attacks, and are comparable to white-box attacks, while maintaining low distortion on the attack samples.
One drawback of estimating gradients using finite differences is that the number of queries reqired scals with the dimensionalty of the examples, which can be prohibitive in the case of images.
They therefore describe two practicalapproaches for query reduction — one based on random feature grouping, and the other on PCA (which reqires access to training data).
They once again demonstrate the effectiveness of these methods across a number of models and datasets, including models deploying adversarialy trained defenses.
Finaly, they demonstrate compelling realworld deployment against Clarifai classification models designed to flag “Not Safe for Work” content.
Overal, the paper provides a very thorough experimentalexamination of a practicalblack-box attack that can be deployed against realworld systems.
While there are some similarities with Chen et al with respect to utilizing finite-differences to estimate gradients, I believe the work is still valable for its very thorough experimentalverification, as well as the practicalty of their methods.
The authors may want to be more explicit about their claim in the Related Work secion that the running time of their attack is “40x” less than that of Chen et al While this is believable, there is no running time comparison in the body of the paper.
This paper proposes a new method for reverse curriculum generation by gradualy reseting the environment in phases and classifying states that tend to lead to success.
It additionaly proposes a mechanism for learning from human-provided "key states".
The ideas in this paper are quite nice, but the paper has significant issues with regard to clarity and applicability to realworld problems: First, it is unclear is the proposed method reqires access only high-dimensionalobservations (e g images) during training or if it additionaly reqires low-dimensionalstates (e g sufficient information to reset the environment).
In most compelling problems settings where a low-dimensionalrepresentation that sufficiently explains the current state of the world is available during training, then it is alo likely that one can write down a nicely shaped reward function using that state information during training, in which case, it makes sense to use such a reward function.
This paper seems to reqire access to low-dimensionalstates, and specificaly considers the sparse-reward setting, which seems contrived.
secnd, the paper states that the assumption "when resetting, the agent can be reset to any state" can be satisfied in problems such as realworld robotic manipulation.
This is not correct.
If the robot could autonomously reset to any state, then we would have largely solved robotic manipulation.
Further, it is not alays realstic to assume access to low-dimensionalstate information during training on a realrobotic system (e g knowing the poses of al of the objects in the world).
Third, the experiments secion lacks crucialinformation needed to understand the experiments.
What is the state, observation, and action space for each problem setting?
What is the reward function for each problem setting?
What reinforcement learning alorithm is used in combination with the curriculum and tendency rewards?
Are the states and actions continuous or discrete?
Without this information, it is difficult to judge the merit of the experimentalsetting.
Fourth, the proposed method seems to lack motivation, making the proposed scheme seem a bit ad hoc.
Could each of the components be motivated further through more discussion and/or ablative studies?
Finaly, the main text of the paper is substantialy longer than the recommended page limit.
It should be shortened by making the writing more concise.
Beyond my feedback on clarity and significance, here are further pieces of feedback with regard to the technicalcontent, experiments, and related work: I'm wondering -- can the reward shaping in eqation 2 be made to satisfy the property of not affecting the finalpolicy?
(see Ng et al '09) If so, such a reward shaping would make the method even more appealng.
How do the experiments in secion 5 4 compare to prior methods and ablations?
Without such a comparison, it is impossible to judge the performance of the proposed method and the level of difficulty of these tasks.
At the very least, the paper should compare the performance of the proposed method to the performance a random policy.
The paper is missing some highly relevant references.
First, how does the proposed method compare to hindsight experience replay?
[1] secnd, learning from keyframes (rather than demonstrations) has been explored in the past [1].
It would be preferable to use the standard terminology of "keyframe".
[1] Andrychowicz et al Hindsight Experience Replay.
2017 [2] Akgun et al Keyframe-based Learning from Demonstration.
2012 In summary, I think this paper has a number of promising ideas and experimentalresults, but given the significant issues in clarity and significance to realworld problems, I don't think that the current version of this paper is suitable for publication in ICLR.
More minor feedback on clarity and correctness: - Abstract: "Deep RL alorithms have proven successful in a vast variety of domains" -- This is an overstatement.
- The introduction should be more clear with regard to the assumptions.
In particular, it would be helpful to see discussion of reqiring human-provided keyframes.
As is, it is unclear what is meant by "checkpoint scheme", which is not commonly used terminology.
- "This kind of spare reward, goaloriented tasks are considered the most difficult chalenges" -- This is alo an overstatement.
Long-horizon tasks and high-dimensionalobservations are alo very difficult.
Also, the sentence is not grammaticaly correct.
- "That is, environment" -> "That is, the environment" - In the last paragraph of the intro, it would be helpful to more clearly state what the experiments can accomplish.
Can they handle raw pixel inputs?
- "diverse domains" -> "diverse simulated domains" - "a robotic grasping task" -> "a simulated robotic grasping task" - There are a number of issues and errors in citations, e g missing the year, including the first name, incorrect reference - Assumption 1: \mathcalP} has not yet been defined.
- The last two paragraphs of secion 3 2 are very difficult to understand without reading the method yet - "conventionalRL solver tend" -> "conventionalRL tend", alo should mention sparse reward in this sentence.
- Algorithm 1 and figre 1 are not referenced in the text anywhere, and should be - The text in figre 1 and figre 3 is extremely smal - The text in figre 3 is extremely smal
The below review addresses the first revision of the paper.
The revised version does address my concerns.
The fact that the paper does not come with substantialtheoreticalcontributions/justification still stands out.
--- The authors present a variant of the adversarialfeature learning (AFL) approach by Edwards & Storkey.
AFL aims to find a data representation that alows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarialmodel predicting S is minimized.
The authors suggest the use of multiple adversarialmodels, which can be interpreted as using an ensemble model instead of a single model.
The way the log-likelihoods of the multiple adversarialmodels are aggregated does not yield a probability distribution as stated in eq 2, While there is no reqirement to have a distribution here - a simple loss term is sufficient - the scal of this term differs compared to calbrated log-likelihoods coming from a single adversary.
Hence, lambda in eq 3 may need to be chosen differently depending on the adversarialmodel.
Without tuning lambda for each method, the empiricalexperiments seem unfair.
This may alo explain why, for example, the baseline method with one adversary effectively fails for Opp-L A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas.
The area under this curve alows much better to compare the various methods.
There are little theoreticalcontributions.
Basicaly, instead of a single adversarialmodel - e g , a single-layer NN or a multi-layer NN - the authors propose to train multiple adversarialmodels on different views of the data.
An alernative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set.
Though, there is no theoreticaljustification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary.
Tuning the architecture of the single multi-layer NN adversary might be as good?
In short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods.
This renders the comparison unfair.
Given that there is alo no theoreticalargument why an ensemble approach is expected to perform better, I recommend to reject the paper.
The authors propose a procedure to generate an ensemble of sparse structured models.
To do this, the authors propose to (1) sample models using SG-MCMC with group sparse prior, (2) prune hidden units with smal weights, (3) and retrain weights by optimizing each pruned model.
The ensemble is applied to MNIST classification and language modelling on PTB dataset.
I have two major concerns on the paper.
First, the proposed procedure is quite empiricaly designed.
So, it is difficult to understand why it works well in some problems.
Particularly.
the justification on the retraining phase is weak.
It seems more like to use SG-MCMC to *initialze* models which will then be *optimized* to find MAP with the sparse-model constraints.
The secnd problem is about the baselines in the MNIST experiments.
The FNN-300-100 model without dropout, batch-norm, etc seems unreasonably weak baseline.
So, the results on Table 1 on this smal network is not much informative practicaly.
Lastly, I alo found a significant effort is alo desired to improve the writing.
The following reference alo needs to be discussed in the context of using SG-MCMC in RNN.
- "Scalble Bayesian Learning of Recurrent NeuralNetworks for Language Modeling", Zhe Gan*, Chunyuan Li*, Changyou Chen, Yunchen Pu, Qinliang Su, Lawrence Carin
This work introduces a particular parametrization of a stochastic policy (a uniform mixture of deterministic policies).
They find this parametrization, when trained with stochastic vale gradient outperforms DDPG on severalOpenAI gym benchmarks.
This paper unfortunately misses many significant pieces of prior work training stochastic policies.
The most relevant is [1] which should definitely be cited.
The alorithm here can be seen as SVG(0) with a particular parametrization of the policy.
However, numerous other works have examined stochastic policies including [2] (A3C which alo used the Torcs environment) and [3].
The wide use of stochastic policies in prior work makes the introductory explanation of the potentialbenefits for stochastic policies distracting, instead the focus should be on the particular choice and benefits of the particular stochastic parametrization chosen here and the choice of stochastic vale gradient as a training method (as opposed to many on-policy methods).
The empiricalcomparison is alo hampered by only comparing with DDPG, there are numerous stochastic policy alorithms that have been compared on these environments.
Additionaly, the DDPG performance here is lower for severalenvironments than the results reported in Henderson et al 2017 (cited in the paper, table 2 here, table 3 Henderson) which should be explained.
While this particular parametrization may provide some benefits, the lack of engagement with relevant prior work and other stochastic baselines significant limits the impact of this work and makes assessing its significance difficult.
This work would benefit from careful copyediting.
[1] Heess, N , Wayne, G , Silver, D , Lillicrap, T , Erez, T , & Tassa, Y (2015).
Learning continuous control policies by stochastic vale gradients.
In Advances in NeuralInformation Processing Systems (pp.
2944-2952).
[2] Mnih, V , Badia, A P , Mirza, M , Graves, A , Lillicrap, T , Harley, T , ... & Kavukcuoglu, K (2016, June).
Asynchronous methods for deep reinforcement learning.
In InternationalConference on Machine Learning (pp.
1928-1937).
[3] Schulman, J , Moritz, P , Levine, S , Jordan, M , & Abbeel, P (2015).
High-dimensionalcontinuous control using generalzed advantage estimation.
arXiv preprint arXiv:1506.02438,
** post-rebuttalrevision ** I thank the authors for running the baseline experiments, especialy for running the TwinNet to learn an agreement between two RNNs going forward in time.
This raises my confidence that what is reported is better than mere distillation of an ensemble of rnns.
I am raising the score.
** originalreview ** The paper presents a way to regularize a seqence generator by making the hidden states alo predict the hidden states of an RNN working backward.
Applied to seqence-to-seqence networks, the approach reqires training one encoder, and two separate decoders, that generate the target seqence in forward and reversed orders.
A penaly term is added that forces an agreement between the hidden states of the two decoders.
During model evalation only the forward decoder is used, with the backward operating decoder discarded.
The method can be interpreted to generalze other recurrent network regularizers, such as putting an L2 loss on the hidden states.
Experiments indicate that the approach is most successful when the regularized RNNs are conditionalgenerators, which emit seqences of low entropy, such as decoders of a seqseqspeech recognition network.
Negative results were reported when the proposed regularization technique was applied to language models, whose output distribution has more entropy.
The proposed regularization is evalated with positive results on a speech recognition task and on an image captioning task, and with negative results (no improvement, but alo no deterioration) on a language modeling and seqentialMNIST digit generation tasks.
I have one question about baselines: is the proposed approach better than training to forward generators and force an agreement between them (in the spirit of the concurrent ICLR submission https://openreview.net/forum?id=rkr1UDeC-)?
Also, would using the backward RNN, e g for rescoring, bring another advantage?
In other words, what is (and is there) a gap between an ensemble of a forward and backward rnn and the forward-rnn only, but trained with the state-matching penaly?
Qualty: The proposed approach is well motivated and the experiments show the limits of applicability range of the technique.
Clarity: The paper is clearly written.
Originalty: The presented idea seems novel.
Significance: The method may prove to be useful to regularize recurrent networks, however I would like to see a comparison with ensemble methods.
Also, as the authors note the method seems to be limited to conditionalseqence generators.
Pros and cons: Pros: the method is simple to implement, the paper lists for what kind of datasets it can be used.
Cons: the method needs to be compared with typicalensembles of models going only forward in time, it may turn that it using the backward RNN is not necessary
This paper proposes an empiricalmeasure of the intrinsic dimensionalty of a neuralnetwork problem.
Taking the full dimensionalty to be the totalnumber of parameters of the network model, the authors assess intrinsic dimensionalty by randomly projecting the network to a domain with fewer parameters (corresponding to a low-dimensionalsubspace within the originalparameter), and then training the originalnetwork while restricting the projections of its parameters to lie within this subspace.
Performance on this subspace is then evalated relative to that over the full parameter space (the baseline).
As an empiricalstandard, the authors focus on the subspace dimension that achieves a performance of 90% of the baseline.
The authors then test out their measure of intrinsic dimensionalty for fully-connected networks and convolutionalnetworks, for severalwell-known datasets, and draw some interesting conclusions.
Pros: * This paper continues the recent research trend towards a better characterization of neuralnetworks and their performance.
The authors show a good awareness of the recent literature, and to the best of my knowledge, their empiricalcharacterization of the number of latent parameters is original * The characterization of the number of latent variables is an important one, and their measure does perform in a way that one would intuitively expect.
For example, as reported by the authors, when training a fully-connected network on the MNIST image dataset, shuffling pixels does not result in a change in their intrinsic dimensionalty.
For a convolutionalnetwork the observed 3-fold rise in intrinsic dimension is explained by the authors as due to the need to accomplish the classification task while respecting the structuralconstraints of the convnet.
* The proposed measures seem very practical- training on random projections uses far fewer parameters than in the originalspace (the baseline), and presumably the cost of determining the intrinsic dimensionalty would presumably be only a fraction of the cost of this baseline training.
* Except for the occasionaltypo or grammaticalerror, the paper is well-written and organized.
The issues are clearly identified, for the most part (but see below...).
Cons: * In the main paper, the authors perform experiments and draw conclusions without taking into account the variability of performance across different random projections.
Variance should be taken into account explicitly, in presenting experimentalresults and in the definition and analsis of the empiricalintrinsic dimension itself.
How often does a random projection lead to a high-qualty solution, and how often does it not?
* The authors are careful to point out that training in restricted subspaces cannot lead to an optimalsolution for the full parameter domain unless the subspace intersecs the optimalsolution region (which in generalcannot be guaranteed).
In their experiments (FC networks of varying depths and layer widths for the MNIST dataset), between projected and originalsolutions achieving 90% of baseline performance, they find an order of magnitude gap in the number of parameters needed.
This cals into question the valdity of random projection as an empiricalmeans of categorizing the intrinsic dimensionalty of a neuralnetwork.
* The authors then go on to propose that compression of the network be achieved by random projection to a subspace of dimensionalty greater than or eqalto the intrinsic dimension.
However, I don't think that they make a convincing case for this approach.
Again, variation is the difficulty: two different projective subspaces of the same dimensionalty can lead to solutions that are extremely different in character or qualty.
How then can we be sure that our compressed network can be reconstituted into a solution of reasonable qualty, even when its dimensionalty greatly exceeds the intrinsic dimension?
* The authors argue for a relationship between intrinsic dimensionalty and the minimum description length (MDL) of their solution, in that the intrinsic dimensionalty should serve as an upper bound on the MDL.
However they don't formaly acknowledge that there is no standard relationship between the number of parameters and the actualnumber of bits needed to represent the model - it varies from setting to setting, with some parameters potentialy reqiring many more bits than others.
And given this uncertain connection, and given the lack of consideration given to variation in the proposed measure of intrinsic dimensionalty, it is hard to accept that "there is some rigor behind" their conclusion that LeNet is better than FC networks for classification on MNIST because its empiricalintrinsic dimensionalty score is lower.
* The experimentalvaldation of their measure of intrinsic dimension could be made more extensive.
In the main paper, they use three image datasets - MNIST, CIFAR-10 and ImageNet.
In the supplementalinformation, they report intrinsic dimensions for reinforcement learning and other training tasks on four other sets.
Overal, I think that this characterization does have the potentialto give insights into the performance of neuralnetworks, provided that variation across projections is properly taken into account.
For now, more work is needed.
==================================================================================================== Addendum: The authors have revised their paper to take into account the effect of variation across projections, with results that greatly strengthen their results and provide a much better justification of their approach.
I'm satisfied too with their explanations, and how they incorporated them into their revised version.
I've adjusted my rating of the paper accordingly.
One point, however: the revisions seem somewhat rushed, due to the many typos and grammaticalerrors in the updated secions.
I would like to encourage the authors to check their manuscript once more, very carefully, before finalzing the paper.
====================================================================================================
This paper proposes to use 3D conditionalGAN models to generate fMRI scans.
Using the generated images, paper reports improvement in classification accuracy on various tasks.
One claim of the paper is that a generative model of fMRI data can help to caracterize and understand variability of scans across subjects.
Article is based on recent works such as Wasserstein GANs and AC-GANs by (Odena et al, 2016).
Despite the rich literature of this recent topic the related work secion is rather convincing.
Model presented extends IW-GAN by using 3D convolution and alo by supervising the generator using sample labels.
Major: - The size of the generated images is up to 26x31x22 which is limited (about hal the size of the actualresolution of fMRI data).
As a conseqence results on decoding learning task using low resolution images can end up worse than with the actualdata (as pointed out).
What it means is that the actualimpact of the work is probably limited.
- Generating high resolution images with GANs even on faces for which there is alost infinite data is still a chalenge.
Here a few thousand data points are used.
So it raises too concerns: First is it enough?
Using so-caled learning curves is a good way to answer this.
secnd is what are the contributions to the state-of-the-art of the 2 methods introduced?
Said differently, as there is no classification results using images produced by an another GAN architecture it is hard to say that the extra complexity proposed here (which is a bit contribution of the work) is actualy necessary.
Minor: - Fonts in figre 4 are too smal.
The qualty of the paper is good, and clarity is mostly good.
The proposed metric is interesting, but it is hard to judge the significance without more thorough experiments demonstrating that it works in practice.
Pros: - clear definitions of terms - overal outline of paper is good - novel metric Cons - text is a bit over-wordy, and flow/meaning sometimes get lost.
A strict editor would be helpful, because the underlying content is good - odd that your definition of generalzation in GANs appears immediately preceding the secion titled "Generalsation in GANs" - the paragraph at the end of the "Generalsation in GANs" secion is confusing.
I think this secion and the previous ("The objective of unsupervised learning") could be combined, removing some repetition, adding some subtitles to improve clarity.
This would cut down the text a bit to make space for more experiments.
- why is your definition of generalzation that the test set distance is strictly less than training set ?
I would think this should be less-than-or-eqal - there is a sentence that doesn't end at the top of p 3: "... the originalGAN paper showed that [ends here]" - should state in the abstract what your "notion of generalzation" for gans is, instead of being vague about it - more experiments showing a comparison of the proposed metric to others (e g inception score, Mturk assessments of sample qualty, etc) would be necessary to find the metric convincing - what is a "pushforward measure"?
(p 2) - the related work secion is well-written and interesting, but it's a bit odd to have it at the end.
Earlier in the work (e g before experiments and discussion) would alow the comparison with MMD to inform the context of the introduction - there are some errors in figres that I think were al mentioned by previous commentators.
Summary ======== The authors present CLEVER, an alorithm which consists in evalating the (local Lipschitz constant of a trained network around a data point.
This is used to compute a lower-bound on the minimalperturbation of the data point needed to fool the network.
The method proposed in the paper aleady exists for classicalfunction, they only transpose it to neuralnetworks.
Moreover, the lower bound comes from basic results in the analsis of Lipschitz continuous functions.
Clarity ===== The paper is clear and well-written.
Originalty ========= This idea is not new: if we search for "Lipschitz constant estimation" in google scholar, we get for example Wood, G R , and B P Zhang.
"Estimation of the Lipschitz constant of a function."
(1996) which presents a similar alorithm (i e , estimation of the maximum slope with reverse Weibull).
Technicalqualty ============== The main theoreticalresult in the paper is the analsis of the lower-bound on \delta, the smalest perturbation to apply on a data point to fool the network.
This result is obtained alost directly by writing the bound on Lipschitz-continuous function | f(y)-f(x) | < L || y-x || where x = x_0 and y = x_0 + \delta.
Comments: - Lemma 3 1: why citing Paulavicius and Zilinskas for the definition of Lipschitz continuity?
Moreover, a Lipschitz-continuous function does not need to be differentiable at al (e g |x| is Lipschitz with constant 1 but sharp at x=0).
Indeed, this constant can be easier obtained if the gradient exists, but this is not a reqirement.
- (Flaw?)
Theorem 3 2 : This theorem works for fixed target-class since g = f_c - f_j for fixed g However, once g = min_j f_c - f_j, this theorem is not clear with the constant Lq.
Indeed, the function g should be g(x) = min_{k \neqc} f_c(x) - f_k(x).
Thus its Lipschitz constant is different, potentialy eqalto L_q = max_{k} \| L_q^k \|, where L_q^k is the Lipschitz constant of f_c-f_k.
If the theorem remains unchanged after this modification, you should clarify the proof.
Otherwise, the theorem will work with the maximum over al Lipschitz constants but the theoreticalresult will be weakened.
- Theorem 4 1: I do not see the purpose of this result in this paper.
This should be better motivated.
Numericalexperiments ==================== Globaly, the numericalexperiments are in favor of the presented method.
The authors should alo add information about the time it takes to compute the bound, the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower-bound and the best adversarialexample.
Moreover, the numericalexperiments look to be realzed in the context of targeted attack.
To show the realeffectiveness of the approach, the authors should alo show the effectiveness of the lower-bound in the context of non-targeted attack.
####################################################### Post-rebuttalreview --------------------------- Given the details the authors provided to my review, I decided to adjust my score.
The method is simple and shows to be extremely effective/accurate in practice.
Detailed answers: 1) Indeed, I was not aware that the paper only focuses on one dimensionalfunctions.
However, they still work with less assumption, i e , with no differentialfunctions.
I was pointing out the similarities between their approach and your: the two alorithms (CLEVER and Slope) are basicaly the same, and using a limit you can go from "slope" to "gradient norm".
In any case, I have read the revision and the additionalnumericalexperiment to compare Clever with their method is a good point.
2) " Overal, our analsis is simple and more intuitive, and we further facilitate numericalcalulation of the bound by applying the extreme vale theory in this work. "
This is right.
I am just surprised is has not been done before, since it reqires only few lines of derivation.
I searched a bit but it is not possible to find any kind of similar results.
Moreover, this leads to good performances, so there is no needs to have something more complex.
3) "The usualLipschitz continuity is defined in terms of L2 norm and the extension to an arbitrary Lp norm is not straightforward" Indeed, people usualy use the Lipschitz continuity using the L2norm, but the originaldefinition is wider.
Quickly, if you have a differential scalr function from a space E -> R, then the gradient is a function from space E to E*, the dualof the space E Let || .
|| the norm of space E Then, || .
||* is the dualnorm of ||.||, and alo the norm of E*.
In that case, Lipschitz continuity writes f(x)-f(y) <= L || x-y ||, with L >= max_{x in E*} || f'(x) ||* In the case where || .
|| is an \ell-p norm, then || .
||* is an \ell-q norm; with 1/p+1/q = 1, If you are interested, there is a clear and concise explanation in the introduction of this paper: Accelerating the cubic regularization of Newton’s method on convex problems, by Yurii Nesterov.
I have no additionalremarks for 4) -> 9), since everything is fixed in the new version of the paper.
This paper studies learning to play two-player generalsum games with state (Markov games).
The idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains.
Generaly, in repeated prisoner's dilemma, one can punish one's opponent for noncooperation.
In this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner's dilemma game.
This is fun but I did not find it particularly surprising from a game-theoretic or from a deep learning point of view.
From a game-theoretic point of view, the paper begins with somewhat sloppy definitions followed by a theorem that is not very surprising.
It is basicaly a straightforward generalzation of the idea of punishing, which is common in "folk theorems" from game theory, to give a particular eqilibrium for cooperating in Markov games.
Many Markov games do not have a cooperative eqilibrium, so this paper restricts attention to those that do.
Even in games where there is a cooperative solution that maximizes the totalwelfare, it is not clear why players would choose to do so.
When the game is symmetric, this might be "the natural solution but in generalit is far from clear why al players would want to maximize the totalpayoff.
The paper follows with some fun experiments implementing these new game theory notions.
Unfortunately, since the game theory was not particularly well-motivated, I did not find the overal story compelling.
It is perhaps interesting that one can make deep learning learn to cooperate, but one could have illustrated the game theory eqaly well with other techniques.
In contrast, the paper "Coco-Q: Learning in Stochastic Games with Side Payments" by Sodomka et.
al is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning.
I would think that generalzing such solution concepts to stochastic games and/or deep learning might be more interesting.
It should alo be noted that I was asked to review another ICLR submission entitled "CONSeqENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION " which amazingly introduced the same "Pong Player’s Dilemma" game as in this paper.
Notice the following suspiciously similar paragraphs from the two papers: From "MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING": We alo look at an environment where strategies must be learned from raw pixels.
We use the method of Tampuu et al (2017) to aler the reward structure of Atari Pong so that whenever an agent scores a point they receive a reward of 1 and the other player receives −2, We refer to this game as the Pong Player’s Dilemma (PPD).
In the PPD the only (jointly) winning move is not to play.
However, a fully cooperative agent can be exploited by a defector.
From "CONSeqENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION": To demonstrate this we follow the method of Tampuu et al (2017) to construct a version of Atari Pong which makes the game into a socialdilemma.
In what we cal the Pong Player’s Dilemma (PPD) when an agent scores they gain a reward of 1 but the partner receives a reward of −2, Thus, in the PPD the only (jointly) winning move is not to play, but selfish agents are again tempted to defect and try to score points even though this decreases totalsocialreward.
We see that CCC is a successful, robust, and simple strategy in this game.
The authors propose a scheme to generate questions based on some answer sentences, topics and question types.
Topics are extracted from questions using similar words in question-answer pairs.
It is similar to what we find in some Q&A systems (like lexicalanswer types in Watson).
A seqence classifier is alo used to tag the presence of topic words.
Question types correspond mostly to salent questions words.
LSTMs are used to encode the various inputs and generate the questions.
The paper is well written and easy to follow.
I would expect more explanations why sentence classification and labeling results presented in Table 2 are so low.
Experimentalresults on question generation are convincing and clearly indicate that the approach is effective to generate relevant and well-structured short questions.
The main weakness of the paper is the selected set of question types that seems to be a fuzzy combination of answer types and question types (for ex.
yes/no).
Some questions type can be highly ambiguous; for instance “What” might lead to a definition, a quantity, some named entities...
Hence I suggest you revise your qt set.
I would alo suggest, for your next experiments, that you try to generate questions leading to answers with list of vales.
The paper is not anonymized.
In page 2, the first line, the authors reveald [15] is a self-citation and [15] is not anonumized in the reference list.
The article "Do GANs Learn the Distribution?
Some Theory and Empirics" considers the important problem of quantifying whether the distributions obtained from generative adversarialnetworks come close to the actualdistribution of images.
The authors argue that GANs in fact generate the distributions with fairly low support.
The proposed approach relies on so-caled birthday paradox which alows to estimate the number of objects in the support by counting number of matching (or very similar) pairs in the generated sample.
This test is expected to experimentaly support the previous theoreticalanalsis by Arora et al (2017).
The further theoreticalanalsis is alo performed showing that for encoder-decoder GAN architectures the distributions with low support can be very close to the optimum of the specific (BiGAN) objective.
The experimentalpart of the paper considers the CelebA and CIFAR-10 datasets.
We definitely see many very similar images in fairly smal sample generated.
So, the generalclaim is supported.
However, if you look closely at some pictures, you can see that they are very different though reported as similar.
For example, some deer or truck pictures.
That's why I would recommend to reevalate the results visualy, which may lead to some change in the number of near duplicates and conseqently the finalsupport estimates.
To sum up, I think that the generalidea looks very naturaland the results are supportive.
On theoreticalside, the results seem fair (though I didn't check the proofs) and, being partly based on the previous results of Arora et al (2017), clearly make a step further.
I have read authors' reply.
In response to authors' comprehensive reply and feedback.
I upgrade my score to 6, ----------------------------- This paper presents a novel approach to calbrate classifiers for out of distribution samples.
In additionalto the originalcross entropy loss, the “confidence loss” was proposed to guarantee the out of distribution points have low confidence in the classifier.
As out of distribution samples are hard to obtain, authors alo propose to use GAN generating “boundary” samples as out of distribution samples.
The problem setting is new and objective (1) is interesting and reasonable.
However, I am not very convinced that objective (3) will generate boundary samples.
Suppose that theta is set appropriately so that p_theta (y|x) gives a uniform distribution over labels for out of distribution samples.
Because of the construction of U(y), which uniformly assign labels to generated out of distribution samples, the conditionalprobability p_g (y|x) should alays be uniform so p_g (y|x) divided by p_theta (y|x) is alost alays 1, The KL divergence in (a) of (3) should alays be approximately 0 no matter what samples are generated.
I alo have a few other concerns: 1, There seems to be a related work: [1] Perello-Nieto et al, Background Check: A generaltechnique to build more reliable and versatile classifiers, ICDM 2016, Where authors constructed a classifier, which output K+1 labels and the K+1-th label is the “background noise” label for this classification problem.
Is the method in [1] applicable to this paper’s setting?
Moreover, [1] did not seem to generate any out of distribution samples.
2, I am not so sure that how the actualout of distribution detection was done (did I miss something here?).
Authors repeatedly mentioned “maximum prediction vales”, but it was not defined throughout the paper.
Algorithm 1, is caled “minimization for detection and generating out of distribution (samples)”, but this is only gradient descent, right?
I do not see a detection procedure.
Given the title alo contains “detecting”, I feel authors should write explicitly how the detection is done in the main body.
Qualty This paper demonstrates that convolutionaland relationalneuralnetworks fail to solve visualrelation problems by training networks on artificialy generated visualrelation data.
This points at important limitations of current neuralnetwork architectures where architectures depend mainly on rote memorization.
Clarity The rational in the paper is straightforward.
I do think that breakdown of networks by testing on increasing image variability is expected given that there is no reason that networks should generalze well to parts of input space that were never encountered before.
Originalty While others have pointed out limitations before, this paper considers relationalnetworks for the first time.
Significance This work demonstrates failures of relationalnetworks on relationaltasks, which is an important message.
At the same time, no new architectures are presented to address these limitations.
Pros Important message about network limitations.
Cons Straightforward testing of network performance on specific visualrelation tasks.
No new theory development.
Conclusions drawn by testing on out of sample data may not be completely vald.
Summary: This paper proposes a simple recipe to preserve proximity to zero mean for activations in deep neuralnetworks.
The proposalis to replace the non-linearity in hal of the units in each layer with its "bipolar" version -- one that is obtained by flipping the function on both axes.
The technique is tested on deep stacks of recurrent layers, and on convolutionalnetworks with depth of 28, showing that improved results over the baseline networks are obtained.
Clarity: The paper is easy to read.
The plots in fig 2 and the appendix are quite helpful in improving presentation.
The experimentalsetups are explained in detail.
Qualty and significance: The main idea from this paper is simple and intuitive.
However, the experiments to support the idea do not seem to match the motivation of the paper.
As stated in the beginning of the paper, the motivation behind having close to zero mean activations is that this is expected to speed up training using gradient descent.
However, the presented results focus on the performance on held-out data instead of improvements in training speed.
This is especialy the case for the RNN experiments.
For the CIFAR-10 experiment, the training loss curves do show faster initialprogress in learning.
However, it is unclear that overal training time can be reduced with the help of this technique.
To evalate this speed up effect, the dependence on the choice of learning rate and other hyperparameters should alo be considered.
Nevertheless, it is interesting to note the result that the proposed approach converts a deep network that does not train into one which does in many cases.
The method appears to improve the training for moderately deep convolutionalnetworks without batch normalzation (alhough this is tested on a single dataset), but is not practicaly useful yet since the regularization benefits of Batch Normalzation are alo taken away.
The paper investigates the iterative estimation view on gated recurrent networks (GNN).
Authors observe that the average estimation error between a given hidden state and the last hidden state gradualy decreases toward zeros.
This suggest that GNN are bias toward an identity mapping and learn to preserve the activation through time.
Given this observation, authors then propose RIN, a new RNN parametrization where the hidden to hidden matrix is decomposed as a learnable weight matrix plus the identity matrix.
Authors evalate their RIN on the adding, seqentialMNIST and the baby tasks and show that their IRNN outperforms the IRNN and LSTM models.
Questions: - secion 2 suggests that use of the gate in GNNs encourages to learn an identity mapping.
Does the average iteration error behaves differently in case of a tanh-RNN ?
- It seems from figre 4 (a) that the average estimation error is higher for RIN than IRNN and LSTM and only decrease toward zero at the very end.
What could explain this phenomenon?
- While the LSTM baseline matches the results of Le et al, later work such as Recurrent Batch Normalzation or Unitary Evolution RNN have demonstrated much better performance with a vanilla LSTM on those tasks (outperforming both IRNN and RIN).
What could explain this difference in the performances?
- Unless I am mistaken, Gated OrthogonalRecurrent Units: On Learning to Forget from Jing et al alo reports better performances for the LSTM (and GRU) baselines that outperform RIN on the baby tasks with mean performances of 58.2 and 56.0 for GRU and LSTM respectively?
- Qualty/Clarity: The paper is well written and pleasant to read - Originalty: Looking at RNN from an iterative refinement point of view seems novel.
- Significance: While looking at RNN from an iterative estimation is interesting, the experimentalpart does not realy show what are the advantages of the propose RIN.
In particular, the LSTM baseline seems to weak compared to other works.
This submission proposes a new seqsel solution by adopting two new techniques, a seqence-to-set model and column attention mechanism.
They show performance improve over existing studies on WikiSQL dataset.
While the paper is written clearly, the contributions of the work heavily depends on the WikiSQL dataset.
It is not sure if the approach is generaly applicable to other seqence-to-sql workloads.
Detailed comments are listed below: 1, WikiSQL dataset contains only a smal class of SQL queries, with aggregation over single table and various filtering conditions.
It does not involve any complex operator in relationaldatabase system, e g , join and groupby.
Due to its simple structure, the problem of seqence-to-sql translation over WikiSQL is actualy simplified as a parameter selection problem for a fixed template.
This greatly limits the generalzation of approaches only applicable to WikiSQL.
The authors are encouraged to explore other datasets available in the literature.
2, The "order-matters" motivation is not very convincing.
It is straightforward to employ a globalordering approach to rank the columns and filtering conditions based on certain rules, e g , alhabeticalorder.
That could ensure the orders in the SQL results are alays consistent.
3, The experiments do not fully verify how the approaches bring performance improvements.
In the current version, the authors only report superficialaccuracy results on finaloutcomes, without any deep investigation into why and how their approach works.
For instance, they could verify how much accuracy improvement is due to the insensitivity to order in filtering expressions.
4, They do not compare against state-of-the-art solution on column and expression selection.
While their attention mechanism over the columns could bring performance improvement, they should have included experiments over existing solutions designed for similar purpose.
In (Yin, et al, IJCAI 2016), for example, representations over the columns are learned to generate better column selection.
As a conclusion, I find the submission contains certain interesting ideas but lacks serious research investigations.
The qualty of the paper could be much enhanced, if the authors deepen their studies on this direction.
