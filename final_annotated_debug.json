[
  {
    "score": 4,
    "review_text": "The paper basically propose keep using the typical data-augmentation transformations done during training also in evaluation time, to prevent adversarial attacks. In the paper they analyze only 2 random resizing and random padding, but I suppose others like random contrast, random relighting, random colorization, ... could be applicable.\n\nSome of the pros of the proposed tricks is that it doesn't require re-training existing models, although as the authors pointed out re-training for adversarial images is necessary to obtain good results.\n\n\nTypically images have different sizes, however in the Dataset are described as having 299x299x3 size, are all the test images resized before hand? How would this method work with variable size images?\n\nThe proposed defense requires increasing the size of the input images, have you analyzed the impact in performance? Also it would be good to know how robust is the method for smaller sizes.\n\nSection 4.6.2 seems to indicate that 1 pixel padding or just resizing 1 pixel is enough to get most of the benefit, please provide an analysis of how results improve as the padding or size increase. \n\nIn section 5 for the challenge authors used a lot more evaluations per image, could you provide how much extra computation is needed for that model?\n\n",
    "tokenized_review_text": [
      "The paper basicaly propose keep using the typicaldata-augmentation transformations done during training alo in evalation time, to prevent adversarialattacks.",
      "In the paper they analze only 2 random resizing and random padding, but I suppose others like random contrast, random relighting, random colorization, ... could be applicable.",
      "Some of the pros of the proposed tricks is that it doesn't reqire re-training existing models, alhough as the authors pointed out re-training for adversarialimages is necessary to obtain good results.",
      "Typicaly images have different sizes, however in the Dataset are described as having 299x299x3 size, are al the test images resized before hand?",
      "How would this method work with variable size images?",
      "The proposed defense reqires increasing the size of the input images, have you analzed the impact in performance?",
      "Also it would be good to know how robust is the method for smaler sizes.",
      "secion 4 6 2 seems to indicate that 1 pixel padding or just resizing 1 pixel is enough to get most of the benefit, please provide an analsis of how results improve as the padding or size increase.",
      "In secion 5 for the chalenge authors used a lot more evalations per image, could you provide how much extra computation is needed for that model?"
    ],
    "all_annotations": [
      {
        "review_id": "B104VQCgM",
        "annotator": "anno0",
        "importance": 0,
        "originality": 0,
        "method": 1,
        "presentation": 0,
        "interpretation": 0,
        "reproducibility": 0,
        "metareview": "maybe",
        "overall": 3,
        "evidence": 4,
        "constructiveness": 3
      }
    ],
    "conference": "iclr18",
    "review_id": "B104VQCgM",
    "gold_annotation": {
      "review_id": "B104VQCgM",
      "annotator": "anno0",
      "importance": 0,
      "originality": 0,
      "method": 1,
      "presentation": 0,
      "interpretation": 0,
      "reproducibility": 0,
      "metareview": "maybe",
      "overall": 3,
      "evidence": 4,
      "constructiveness": 3
    }
  },
  {
    "score": 3,
    "review_text": "The authors consider new attacks for generating adversarial samples against neural networks. In particular, they are interested in approximating gradient-based white-box attacks such as FGSM in a black-box setting by estimating gradients from queries to the classifier. They assume that the attacker is able to query, for any example x, the vector of probabilities p(x) corresponding to each class.\n\nGiven such query access it\u2019s trivial to estimate the gradients of p using finite differences. As a consequence one can implement FGSM using these estimates assuming cross-entropy loss, as well as a logit-based loss. They consider both iterative and single-step FGSM attacks in the targeted (i.e. the adversary\u2019s goal is to switch the example\u2019s label to a specific alternative label) and un-targeted settings (any mislabelling is a success). They compare themselves to transfer black-box attacks, where the adversary trains a proxy model and generates the adversarial sample by running a white-box attack on that model.  For a number of target classifiers on both MNIST and CIFAR-10, they show that these attacks outperform the transfer-based attacks, and are comparable to white-box attacks, while maintaining low distortion on the attack samples. \n\nOne drawback of estimating gradients using finite differences is that the number of queries required scales with the dimensionality of the examples, which can be prohibitive in the case of images. They therefore describe two practical approaches for query reduction \u2014 one based on random feature grouping, and the other on PCA (which requires access to training data). They once again demonstrate the effectiveness of these methods across a number of models and datasets, including models deploying adversarially trained defenses. \n\nFinally, they demonstrate compelling real-world deployment against Clarifai classification models designed to flag \u201cNot Safe for Work\u201d content. \n\nOverall, the paper provides a very thorough experimental examination of a practical black-box attack that can be deployed against real-world systems. While there are some similarities with Chen et al. with respect to utilizing finite-differences to estimate gradients, I believe the work is still valuable for its very thorough experimental verification, as well as the practicality of their methods. The authors may want to be more explicit about their claim in the Related Work section that the running time of their attack is \u201c40x\u201d less than that of Chen et al. While this is believable, there is no running time comparison in the body of the paper. ",
    "all_annotations": [
      {
        "review_id": "B10Nn-jlf",
        "annotator": "anno0",
        "importance": 1,
        "originality": 1,
        "method": null,
        "presentation": 1,
        "interpretation": 0,
        "reproducibility": 0,
        "metareview": "no",
        "overall": 3,
        "evidence": 3,
        "constructiveness": 3
      }
    ],
    "conference": "iclr18",
    "review_id": "B10Nn-jlf",
    "gold_annotation": {
      "review_id": "B10Nn-jlf",
      "annotator": "anno0",
      "importance": 1,
      "originality": 1,
      "method": null,
      "presentation": 1,
      "interpretation": 0,
      "reproducibility": 0,
      "metareview": "no",
      "overall": 3,
      "evidence": 3,
      "constructiveness": 3
    }
  },
  {
    "score": 1,
    "review_text": "This paper proposes a new method for reverse curriculum generation by gradually reseting the environment in phases and classifying states that tend to lead to success. It additionally proposes a mechanism for learning from human-provided \"key states\".\n\nThe ideas in this paper are quite nice, but the paper has significant issues with regard to clarity and applicability to real-world problems:\nFirst, it is unclear is the proposed method requires access only high-dimensional observations (e.g. images) during training or if it additionally requires low-dimensional states (e.g. sufficient information to reset the environment). In most compelling problems settings where a low-dimensional representation that sufficiently explains the current state of the world is available during training, then it is also likely that one can write down a nicely shaped reward function using that state information during training, in which case, it makes sense to use such a reward function. This paper seems to require access to low-dimensional states, and specifically considers the sparse-reward setting, which seems contrived.\nSecond, the paper states that the assumption \"when resetting, the agent can be reset to any state\" can be satisfied in problems such as real-world robotic manipulation. This is not correct. If the robot could autonomously reset to any state, then we would have largely solved robotic manipulation. Further, it is not always realistic to assume access to low-dimensional state information during training on a real robotic system (e.g. knowing the poses of all of the objects in the world).\nThird, the experiments section lacks crucial information needed to understand the experiments. What is the state, observation, and action space for each problem setting? What is the reward function for each problem setting? What reinforcement learning algorithm is used in combination with the curriculum and tendency rewards? Are the states and actions continuous or discrete? Without this information, it is difficult to judge the merit of the experimental setting.\nFourth, the proposed method seems to lack motivation, making the proposed scheme seem a bit ad hoc. Could each of the components be motivated further through more discussion and/or ablative studies?\nFinally, the main text of the paper is substantially longer than the recommended page limit. It should be shortened by making the writing more concise.\n\nBeyond my feedback on clarity and significance, here are further pieces of feedback with regard to the technical content, experiments, and related work:\nI'm wondering -- can the reward shaping in Equation 2 be made to satisfy the property of not affecting the final policy? (see Ng et al. '09) If so, such a reward shaping would make the method even more appealing.\nHow do the experiments in section 5.4 compare to prior methods and ablations? Without such a comparison, it is impossible to judge the performance of the proposed method and the level of difficulty of these tasks. At the very least, the paper should compare the performance of the proposed method to the performance a random policy.\n\nThe paper is missing some highly relevant references. First, how does the proposed method compare to hindsight experience replay? [1] Second, learning from keyframes (rather than demonstrations) has been explored in the past [1]. It would be preferable to use the standard terminology of \"keyframe\".\n\n[1] Andrychowicz et al. Hindsight Experience Replay. 2017\n[2] Akgun et al. Keyframe-based Learning from Demonstration. 2012\n\nIn summary, I think this paper has a number of promising ideas and experimental results, but given the significant issues in clarity and significance to real world problems, I don't think that the current version of this paper is suitable for publication in ICLR.\n\nMore minor feedback on clarity and correctness:\n- Abstract: \"Deep RL algorithms have proven successful in a vast variety of domains\" -- This is an overstatement.\n- The introduction should be more clear with regard to the assumptions. In particular, it would be helpful to see discussion of requiring human-provided keyframes. As is, it is unclear what is meant by \"checkpoint scheme\", which is not commonly used terminology.\n- \"This kind of spare reward, goal-oriented tasks are considered the most difficult challenges\" -- This is also an overstatement. Long-horizon tasks and high-dimensional observations are also very difficult. Also, the sentence is not grammatically correct.\n- \"That is, environment\" -> \"That is, the environment\"\n- In the last paragraph of the intro, it would be helpful to more clearly state what the experiments can accomplish. Can they handle raw pixel inputs?\n- \"diverse domains\" -> \"diverse simulated domains\"\n- \"a robotic grasping task\" -> \"a simulated robotic grasping task\"\n- There are a number of issues and errors in citations, e.g. missing the year, including the first name, incorrect reference\n- Assumption 1: \\mathcal{P} has not yet been defined.\n- The last two paragraphs of section 3.2 are very difficult to understand without reading the method yet\n- \"conventional RL solver tend\" -> \"conventional RL tend\", also should mention sparse reward in this sentence.\n- Algorithm 1 and Figure 1 are not referenced in the text anywhere, and should be\n- The text in Figure 1 and Figure 3 is extremely small\n- The text in Figure 3 is extremely small\n\n\n",
    "all_annotations": [
      {
        "review_id": "B129GzFxf",
        "annotator": "anno0",
        "importance": 0,
        "originality": 0,
        "method": 1,
        "presentation": 1,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "yes-agree",
        "overall": 5,
        "evidence": 4,
        "constructiveness": 4
      }
    ],
    "conference": "iclr18",
    "review_id": "B129GzFxf",
    "gold_annotation": {
      "review_id": "B129GzFxf",
      "annotator": "anno0",
      "importance": 0,
      "originality": 0,
      "method": 1,
      "presentation": 1,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "yes-agree",
      "overall": 5,
      "evidence": 4,
      "constructiveness": 4
    }
  },
  {
    "score": 3,
    "review_text": "The below review addresses the first revision of the paper. The revised version does address my concerns. The fact that the paper does not come with substantial theoretical contributions/justification still stands out.\n\n---\n\nThe authors present a variant of the adversarial feature learning (AFL) approach by Edwards & Storkey. AFL aims to find a data representation that allows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S. The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarial model predicting S is minimized. The authors suggest the use of multiple adversarial models, which can be interpreted as using an ensemble model instead of a single model.\n\nThe way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq. 2. While there is no requirement to have a distribution here - a simple loss term is sufficient - the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary. Hence, lambda in Eq. 3 may need to be chosen differently depending on the adversarial model. Without tuning lambda for each method, the empirical experiments seem unfair. This may also explain why, for example, the baseline method with one adversary effectively fails for Opp-L. A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas. The area under this curve allows much better to compare the various methods.\n\nThere are little theoretical contributions. Basically, instead of a single adversarial model - e.g., a single-layer NN or a multi-layer NN - the authors propose to train multiple adversarial models on different views of the data. An alternative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set. Though, there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary. Tuning the architecture of the single multi-layer NN adversary might be as good?\n\nIn short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods. This renders the comparison unfair. Given that there is also no theoretical argument why an ensemble approach is expected to perform better, I recommend to reject the paper.",
    "all_annotations": [
      {
        "review_id": "B143HDlWM",
        "annotator": "anno0",
        "importance": 1,
        "originality": 1,
        "method": 1,
        "presentation": 0,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "no",
        "overall": 4,
        "evidence": 5,
        "constructiveness": 4
      }
    ],
    "conference": "iclr18",
    "review_id": "B143HDlWM",
    "gold_annotation": {
      "review_id": "B143HDlWM",
      "annotator": "anno0",
      "importance": 1,
      "originality": 1,
      "method": 1,
      "presentation": 0,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "no",
      "overall": 4,
      "evidence": 5,
      "constructiveness": 4
    }
  },
  {
    "score": 5,
    "review_text": "The authors propose a procedure to generate an ensemble of sparse structured models. To do this, the authors propose to (1) sample models using SG-MCMC with group sparse prior, (2) prune hidden units with small weights, (3) and retrain weights by optimizing each pruned model. The ensemble is applied to MNIST classification and language modelling on PTB dataset. \n\nI have two major concerns on the paper. First, the proposed procedure is quite empirically designed. So, it is difficult to understand why it works well in some problems. Particularly. the justification on the retraining phase is weak. It seems more like to use SG-MCMC to *initialize* models which will then be *optimized* to find MAP with the sparse-model constraints. The second problem is about the baselines in the MNIST experiments. The FNN-300-100 model without dropout, batch-norm, etc. seems unreasonably weak baseline. So, the results on Table 1 on this small network is not much informative practically. Lastly, I also found a significant effort is also desired to improve the writing. \n\nThe following reference also needs to be discussed in the context of using SG-MCMC in RNN.\n- \"Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling\", Zhe Gan*, Chunyuan Li*, Changyou Chen, Yunchen Pu, Qinliang Su, Lawrence Carin",
    "tokenized_review_text": [
      "The authors propose a procedure to generate an ensemble of sparse structured models.",
      "To do this, the authors propose to (1) sample models using SG-MCMC with group sparse prior, (2) prune hidden units with smal weights, (3) and retrain weights by optimizing each pruned model.",
      "The ensemble is applied to MNIST classification and language modelling on PTB dataset.",
      "I have two major concerns on the paper.",
      "First, the proposed procedure is quite empiricaly designed.",
      "So, it is difficult to understand why it works well in some problems.",
      "Particularly.",
      "the justification on the retraining phase is weak.",
      "It seems more like to use SG-MCMC to *initialze* models which will then be *optimized* to find MAP with the sparse-model constraints.",
      "The secnd problem is about the baselines in the MNIST experiments.",
      "The FNN-300-100 model without dropout, batch-norm, etc seems unreasonably weak baseline.",
      "So, the results on Table 1 on this smal network is not much informative practicaly.",
      "Lastly, I alo found a significant effort is alo desired to improve the writing.",
      "The following reference alo needs to be discussed in the context of using SG-MCMC in RNN.",
      "- \"Scalble Bayesian Learning of Recurrent NeuralNetworks for Language Modeling\", Zhe Gan*, Chunyuan Li*, Changyou Chen, Yunchen Pu, Qinliang Su, Lawrence Carin"
    ],
    "all_annotations": [
      {
        "review_id": "B1A7YkceM",
        "annotator": "anno0",
        "importance": 0,
        "originality": 0,
        "method": 1,
        "presentation": 0,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "no",
        "overall": 3,
        "evidence": 4,
        "constructiveness": 3
      }
    ],
    "conference": "iclr18",
    "review_id": "B1A7YkceM",
    "gold_annotation": {
      "review_id": "B1A7YkceM",
      "annotator": "anno0",
      "importance": 0,
      "originality": 0,
      "method": 1,
      "presentation": 0,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "no",
      "overall": 3,
      "evidence": 4,
      "constructiveness": 3
    }
  },
  {
    "score": 3,
    "review_text": "This work introduces a particular parametrization of a stochastic policy (a uniform mixture of deterministic policies). They find this parametrization, when trained with stochastic value gradient outperforms DDPG on several OpenAI gym benchmarks.\n\nThis paper unfortunately misses many significant pieces of prior work training stochastic policies. The most relevant is [1] which should definitely be cited. The algorithm here can be seen as SVG(0) with a particular parametrization of the policy. However, numerous other works have examined stochastic policies including [2] (A3C which also used the Torcs environment) and [3].\n\nThe wide use of stochastic policies in prior work makes the introductory explanation of the potential benefits for stochastic policies distracting, instead the focus should be on the particular choice and benefits of the particular stochastic parametrization chosen here and the choice of stochastic value gradient as a training method (as opposed to many on-policy methods).\n\nThe empirical comparison is also hampered by only comparing with DDPG, there are numerous stochastic policy algorithms that have been compared on these environments. Additionally, the DDPG performance here is lower for several environments than the results reported in Henderson et al. 2017 (cited in the paper, table 2 here, table 3 Henderson) which should be explained.\n\nWhile this particular parametrization may provide some benefits, the lack of engagement with relevant prior work and other stochastic baselines significant limits the impact of this work and makes assessing its significance difficult.\n\nThis work would benefit from careful copyediting.\n\n[1] Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., & Tassa, Y. (2015). Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems (pp. 2944-2952).\n\n[2] Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., ... & Kavukcuoglu, K. (2016, June). Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning (pp. 1928-1937).\n\n[3] Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2015). High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438.\n\n",
    "all_annotations": [
      {
        "review_id": "B1B3e0Oef",
        "annotator": "anno0",
        "importance": 0,
        "originality": 1,
        "method": 1,
        "presentation": 0,
        "interpretation": 0,
        "reproducibility": 0,
        "metareview": "yes-agree",
        "overall": 3,
        "evidence": 3,
        "constructiveness": 4
      }
    ],
    "conference": "iclr18",
    "review_id": "B1B3e0Oef",
    "gold_annotation": {
      "review_id": "B1B3e0Oef",
      "annotator": "anno0",
      "importance": 0,
      "originality": 1,
      "method": 1,
      "presentation": 0,
      "interpretation": 0,
      "reproducibility": 0,
      "metareview": "yes-agree",
      "overall": 3,
      "evidence": 3,
      "constructiveness": 4
    }
  },
  {
    "score": 2,
    "review_text": "** post-rebuttal revision **\n\nI thank the authors for running the baseline experiments, especially for running the TwinNet to learn an agreement between two RNNs going forward in time. This raises my confidence that what is reported is better than mere distillation of an ensemble of rnns. I am raising the score.\n\n** original review **\n\n\nThe paper presents a way to regularize a sequence generator by making the hidden states also predict the hidden states of an RNN working backward.\n\nApplied to sequence-to-sequence networks, the approach requires training one encoder, and two separate decoders, that generate the target sequence in forward and reversed orders. A penalty term is added that forces an agreement between the hidden states of the two decoders. During model evaluation only the forward decoder is used, with the backward operating decoder discarded. The method can be interpreted to generalize other recurrent network regularizers, such as putting an L2 loss on the hidden states.\n\nExperiments indicate that the  approach is most successful when the regularized RNNs are conditional generators, which emit sequences of low entropy, such as decoders of a seq2seq speech recognition network. Negative results were reported when the proposed regularization technique was applied to language models, whose output distribution has more entropy.\n\nThe proposed regularization is evaluated with positive results on a speech recognition task and on an  image captioning task, and with negative results (no improvement, but also no deterioration) on a language modeling and sequential MNIST digit generation tasks.\n\nI have one question about baselines: is the proposed approach better than training to forward generators and force an agreement between them (in the spirit of the concurrent ICLR submission https://openreview.net/forum?id=rkr1UDeC-)? \n\nAlso, would using the backward RNN, e.g. for rescoring, bring another advantage? In other words, what is (and is there) a gap between an ensemble of a forward and backward rnn and the forward-rnn only, but trained with the state-matching penalty?\n\nQuality:\nThe proposed approach is well motivated and the experiments show the limits of applicability range of the technique.\n\nClarity:\nThe paper is clearly written.\n\nOriginality:\nThe presented idea seems novel.\n\nSignificance:\nThe method may prove to be useful to regularize recurrent networks, however I would like to see a comparison with ensemble methods. Also, as the authors note the method seems to be limited to conditional sequence generators.\n\nPros and cons:\nPros: the method is simple to implement, the paper lists for what kind of datasets it can be used.\nCons: the method needs to be compared with typical ensembles of models going only forward in time, it may turn that it using the backward RNN is not necessary\n",
    "all_annotations": [
      {
        "review_id": "B1Fe0Zqxz",
        "annotator": "anno0",
        "importance": 1,
        "originality": 1,
        "method": 1,
        "presentation": 0,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "yes-agree",
        "overall": 4,
        "evidence": 4,
        "constructiveness": 4
      }
    ],
    "conference": "iclr18",
    "review_id": "B1Fe0Zqxz",
    "gold_annotation": {
      "review_id": "B1Fe0Zqxz",
      "annotator": "anno0",
      "importance": 1,
      "originality": 1,
      "method": 1,
      "presentation": 0,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "yes-agree",
      "overall": 4,
      "evidence": 4,
      "constructiveness": 4
    }
  },
  {

    "score": 3,
    "review_text": "This paper proposes an empirical measure of the intrinsic dimensionality of a neural network problem. Taking the full dimensionality to be the total number of parameters of the network model, the authors assess intrinsic dimensionality by randomly projecting the network to a domain with fewer parameters (corresponding to a low-dimensional subspace within the original parameter), and then training the original network while restricting the projections of its parameters to lie within this subspace. Performance on this subspace is then evaluated relative to that over the full parameter space (the baseline). As an empirical standard, the authors focus on the subspace dimension that achieves a performance of 90% of the baseline. The authors then test out their measure of intrinsic dimensionality for fully-connected networks and convolutional networks, for several well-known datasets, and draw some interesting conclusions.\n\nPros:\n\n* This paper continues the recent research trend towards a better characterization of neural networks and their performance. The authors show a good awareness of the recent literature, and to the best of my knowledge, their empirical characterization of the number of latent parameters is original. \n\n* The characterization of the number of latent variables is an important one, and their measure does perform in a way that one would intuitively expect. For example, as reported by the authors, when training a fully-connected network on the MNIST image dataset, shuffling pixels does not result in a change in their intrinsic dimensionality. For a convolutional network the observed 3-fold rise in intrinsic dimension is explained by the authors as due to the need to accomplish the classification task while respecting the structural constraints of the convnet.\n\n* The proposed measures seem very practical - training on random projections uses far fewer parameters than in the original space (the baseline), and presumably the cost of determining the intrinsic dimensionality would presumably be only a fraction of the cost of this baseline training.\n\n* Except for the occasional typo or grammatical error, the paper is well-written and organized. The issues are clearly identified, for the most part (but see below...).\n\nCons:\n\n* In the main paper, the authors perform experiments and draw conclusions without taking into account the variability of performance across different random projections. Variance should be taken into account explicitly, in presenting experimental results and in the definition and analysis of the empirical intrinsic dimension itself. How often does a random projection lead to a high-quality solution, and how often does it not?\n\n* The authors are careful to point out that training in restricted subspaces cannot lead to an optimal solution for the full parameter domain unless the subspace intersects the optimal solution region (which in general cannot be guaranteed). In their experiments (FC networks of varying depths and layer widths for the MNIST dataset), between projected and original solutions achieving 90% of baseline performance, they find an order of magnitude gap in the number of parameters needed. This calls into question the validity of random projection as an empirical means of categorizing the intrinsic dimensionality of a neural network.\n\n* The authors then go on to propose that compression of the network be achieved by random projection to a subspace of dimensionality greater than or equal to the intrinsic dimension. However, I don't think that they make a convincing case for this approach. Again, variation is the difficulty: two different projective subspaces of the same dimensionality can lead to solutions that are extremely different in character or quality. How then can we be sure that our compressed network can be reconstituted into a solution of reasonable quality, even when its dimensionality greatly exceeds the intrinsic dimension?\n\n* The authors argue for a relationship between intrinsic dimensionality and the minimum description length (MDL) of their solution, in that the intrinsic dimensionality should serve as an upper bound on the MDL. However they don't formally acknowledge that there is no standard relationship between the number of parameters and the actual number of bits needed to represent the model - it varies from setting to setting, with some parameters potentially requiring many more bits than others. And given this uncertain connection, and given the lack of consideration given to variation in the proposed measure of intrinsic dimensionality, it is hard to accept that \"there is some rigor behind\" their conclusion that LeNet is better than FC networks for classification on MNIST because its empirical intrinsic dimensionality score is lower.\n\n* The experimental validation of their measure of intrinsic dimension could be made more extensive. In the main paper, they use three image datasets - MNIST, CIFAR-10 and ImageNet. In the supplemental information, they report intrinsic dimensions for reinforcement learning and other training tasks on four other sets.\n\nOverall, I think that this characterization does have the potential to give insights into the performance of neural networks, provided that variation across projections is properly taken into account. For now, more work is needed.\n\n====================================================================================================\nAddendum:\n\nThe authors have revised their paper to take into account the effect of variation across projections, with results that greatly strengthen their results and provide a much better justification of their approach. I'm satisfied too with their explanations, and how they incorporated them into their revised version. I've adjusted my rating of the paper accordingly.\n\nOne point, however: the revisions seem somewhat rushed, due to the many typos and grammatical errors in the updated sections. I would like to encourage the authors to check their manuscript once more, very carefully, before finalizing the paper.\n====================================================================================================",
    "all_annotations": [
      {
        "review_id": "B1IwI-2xz",
        "annotator": "anno0",
        "importance": 1,
        "originality": 1,
        "method": 1,
        "presentation": 0,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "no",
        "overall": 4,
        "evidence": 4,
        "constructiveness": 4
      }
    ],
    "conference": "iclr18",
    "review_id": "B1IwI-2xz",
    "gold_annotation": {
      "review_id": "B1IwI-2xz",
      "annotator": "anno0",
      "importance": 1,
      "originality": 1,
      "method": 1,
      "presentation": 0,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "no",
      "overall": 4,
      "evidence": 4,
      "constructiveness": 4
    }
  },
  {
    "score": 3,
    "review_text": "This paper proposes to use 3D conditional GAN models to generate\nfMRI scans. Using the generated images, paper reports improvement\nin classification accuracy on various tasks.\n\nOne claim of the paper is that a generative model of fMRI\ndata can help to caracterize and understand variability of scans\nacross subjects.\n\nArticle is based on recent works such as Wasserstein GANs and AC-GANs\nby (Odena et al., 2016).\n\nDespite the rich literature of this recent topic the related work\nsection is rather convincing.\n\nModel presented extends IW-GAN by using 3D convolution and also\nby supervising the generator using sample labels.\n\nMajor:\n\n- The size of the generated images is up to 26x31x22 which is limited\n(about half the size of the actual resolution of fMRI data). As a\nconsequence results on decoding learning task using low resolution\nimages can end up worse than with the actual data (as pointed out).\nWhat it means is that the actual impact of the work is probably limited.\n\n- Generating high resolution images with GANs even on faces for which\nthere is almost infinite data is still a challenge. Here a few thousand\ndata points are used. So it raises too concerns: First is it enough?\nUsing so-called learning curves is a good way to answer this. Second\nis what are the contributions to the state-of-the-art of the 2\nmethods introduced? Said differently, as there\nis no classification results using images produced by an another\nGAN architecture it is hard to say that the extra complexity\nproposed here (which is a bit contribution of the work) is actually\nnecessary.\n\nMinor:\n\n- Fonts in figure 4 are too small.\n",
    "all_annotations": [
      {
        "review_id": "B1LfYs_gf",
        "annotator": "anno0",
        "importance": 0,
        "originality": 0,
        "method": 1,
        "presentation": 1,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "yes-agree",
        "overall": 2,
        "evidence": 3,
        "constructiveness": 2
      }
    ],
    "conference": "iclr18",
    "review_id": "B1LfYs_gf",
    "gold_annotation": {
      "review_id": "B1LfYs_gf",
      "annotator": "anno0",
      "importance": 0,
      "originality": 0,
      "method": 1,
      "presentation": 1,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "yes-agree",
      "overall": 2,
      "evidence": 3,
      "constructiveness": 2
    }
  },
  {
    "score": 3,
    "review_text": "The quality of the paper is good, and clarity is mostly good. The proposed metric is interesting, but it is hard to judge the significance without more thorough experiments demonstrating that it works in practice.\n\nPros:\n - clear definitions of terms\n - overall outline of paper is good\n - novel metric\n\nCons\n - text is a bit over-wordy, and flow/meaning sometimes get lost. A strict editor would be helpful, because the underlying content is good\n - odd that your definition of generalization in GANs appears immediately preceding the section titled \"Generalisation in GANs\"\n - the paragraph at the end of the \"Generalisation in GANs\" section is confusing. I think this section and the previous (\"The objective of unsupervised learning\") could be combined, removing some repetition, adding some subtitles to improve clarity. This would cut down the text a bit to make space for more experiments.\n - why is your definition of generalization that the test set distance is strictly less than training set ? I would think this should be less-than-or-equal\n - there is a sentence that doesn't end at the top of p.3: \"... the original GAN paper showed that [ends here]\"\n - should state in the abstract what your \"notion of generalization\" for gans is, instead of being vague about it\n - more experiments showing a comparison of the proposed metric to others (e.g. inception score, Mturk assessments of sample quality, etc.) would be necessary to find the metric convincing\n - what is a \"pushforward measure\"? (p.2)\n - the related work section is well-written and interesting, but it's a bit odd to have it at the end. Earlier in the work (e.g. before experiments and discussion) would allow the comparison with MMD to inform the context of the introduction\n - there are some errors in figures that I think were all mentioned by previous commentators.",
    "all_annotations": [
      {
        "review_id": "B1P-gBclf",
        "annotator": "anno0",
        "importance": 1,
        "originality": 1,
        "method": 0,
        "presentation": 1,
        "interpretation": 0,
        "reproducibility": 0,
        "metareview": "no",
        "overall": 3,
        "evidence": 5,
        "constructiveness": 4
      }
    ],
    "conference": "iclr18",
    "review_id": "B1P-gBclf",
    "gold_annotation": {
      "review_id": "B1P-gBclf",
      "annotator": "anno0",
      "importance": 1,
      "originality": 1,
      "method": 0,
      "presentation": 1,
      "interpretation": 0,
      "reproducibility": 0,
      "metareview": "no",
      "overall": 3,
      "evidence": 5,
      "constructiveness": 4
    }
  },
  {
    "score": 3,
    "review_text": "Summary\n========\n\nThe authors present CLEVER, an algorithm which consists in evaluating the (local) Lipschitz constant of a trained network around a data point. This is used to compute a lower-bound on the minimal perturbation of the data point needed to fool the network.\n\nThe method proposed in the paper already exists for classical function, they only transpose it to neural networks. Moreover, the lower bound comes from basic results in the analysis of Lipschitz continuous functions.\n\n\nClarity\n=====\n\nThe paper is clear and well-written.\n\n\nOriginality\n=========\n\nThis idea is not new: if we search for \"Lipschitz constant estimation\" in google scholar, we get for example\nWood, G. R., and B. P. Zhang. \"Estimation of the Lipschitz constant of a function.\" (1996)\nwhich presents a similar algorithm (i.e., estimation of the maximum slope with reverse Weibull).\n\n\nTechnical quality\n==============\n\nThe main theoretical result in the paper is the analysis of the lower-bound on \\delta, the smallest perturbation to apply on\na data point to fool the network. This result is obtained almost directly by writing the bound on Lipschitz-continuous function\n | f(y)-f(x) | < L || y-x ||\nwhere x = x_0 and y = x_0 + \\delta.\n\nComments:\n- Lemma 3.1: why citing Paulavicius and Zilinskas for the definition of Lipschitz continuity? Moreover, a Lipschitz-continuous function does not need to be differentiable at all (e.g. |x| is Lipschitz with constant 1 but sharp at x=0). Indeed, this constant can be easier obtained if the gradient exists, but this is not a requirement.\n\n- (Flaw?) Theorem 3.2 : This theorem works for fixed target-class since g = f_c - f_j for fixed g. However, once g = min_j f_c - f_j, this theorem is not clear with the constant Lq. Indeed, the function g should be \ng(x) = min_{k \\neq c} f_c(x) - f_k(x).\nThus its Lipschitz constant is different, potentially equal to\nL_q = max_{k} \\| L_q^k \\|, \nwhere L_q^k is the Lipschitz constant of f_c-f_k. If the theorem remains unchanged after this modification, you should clarify the proof. Otherwise, the theorem will work with the maximum over all Lipschitz constants but the theoretical result will be weakened.\n\n- Theorem 4.1: I do not see the purpose of this result in this paper. This should be better motivated.\n\n\nNumerical experiments\n====================\n\nGlobally, the numerical experiments are in favor of the presented method. The authors should also add information about the time it takes to compute the bound, the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower-bound and the best adversarial example.\n\nMoreover, the numerical experiments look to be realized in the context of targeted attack. To show the real effectiveness of the approach, the authors should also show the effectiveness of the lower-bound in the context of non-targeted attack.\n\n\n#######################################################\n\nPost-rebuttal review\n---------------------------\n\nGiven the details the authors provided to my review, I decided to adjust my score. The method is simple and shows to be extremely effective/accurate in practice.\n\nDetailed answers:\n\n1) Indeed, I was not aware that the paper only focuses on one dimensional functions. However, they still work with less assumption, i.e., with no differential functions. I was pointing out the similarities between their approach and your: the two algorithms (CLEVER and Slope) are basically the same, and using a limit you can go from \"slope\" to \"gradient norm\".\nIn any case, I have read the revision and the additional numerical experiment to compare Clever with their method is a good point.\n\n2) \" Overall, our analysis is simple and more intuitive, and we further facilitate numerical calculation of the bound by applying the extreme value theory in this work. \"\nThis is right. I am just surprised is has not been done before, since it requires only few lines of derivation. I searched a bit but it is not possible to find any kind of similar results. Moreover, this leads to good performances, so there is no needs to have something more complex.\n\n3) \"The usual Lipschitz continuity is defined in terms of L2 norm and the extension to an arbitrary Lp norm is not straightforward\"\nIndeed, people usually use the Lipschitz continuity using the L2norm, but the original definition is wider.\nQuickly, if you have a differential, scalar function from a space E -> R, then the gradient is a function from space E to E*, the dual of the space E.\nLet || . || the norm of space E. Then, || . ||* is the dual norm of ||.||, and also the norm of E*.\nIn that case, Lipschitz continuity writes\nf(x)-f(y) <= L || x-y ||, with L >= max_{x in E*} || f'(x) ||*\nIn the case where || . || is an \\ell-p norm, then || . ||* is an \\ell-q norm; with 1/p+1/q = 1.\n\nIf you are interested, there is a clear and concise explanation in the introduction of this paper: Accelerating the cubic regularization of Newton\u2019s method on convex problems, by Yurii Nesterov.\n\nI have no additional remarks for 4) -> 9), since everything is fixed in the new version of the paper.\n\n",
    "all_annotations": [
      {
        "review_id": "B1ZlEVXyf",
        "annotator": "anno0",
        "importance": 1,
        "originality": 1,
        "method": 1,
        "presentation": 0,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "no",
        "overall": 4,
        "evidence": 4,
        "constructiveness": 4
      }
    ],
    "conference": "iclr18",
    "review_id": "B1ZlEVXyf",
    "gold_annotation": {
      "review_id": "B1ZlEVXyf",
      "annotator": "anno0",
      "importance": 1,
      "originality": 1,
      "method": 1,
      "presentation": 0,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "no",
      "overall": 4,
      "evidence": 4,
      "constructiveness": 4
    }
  },
  {
    "score": 3,
    "review_text": "This paper studies learning to play two-player general-sum games with state (Markov games). The idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains. Generally, in repeated prisoner's dilemma, one can punish one's opponent for noncooperation. In this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner's dilemma game. This is fun but I did not find it particularly surprising from a game-theoretic or from a deep learning point of view. \n\nFrom a game-theoretic point of view, the paper begins with somewhat sloppy definitions followed by a theorem that is not very surprising. It is basically a straightforward generalization of the idea of punishing, which is common in \"folk theorems\" from game theory, to give a particular equilibrium for cooperating in Markov games. Many Markov games do not have a cooperative equilibrium, so this paper restricts attention to those that do. Even in games where there is a cooperative solution that maximizes the total welfare, it is not clear why players would choose to do so. When the game is symmetric, this might be \"the natural\" solution but in general it is far from clear why all players would want to maximize the total payoff. \n\nThe paper follows with some fun experiments implementing these new game theory notions. Unfortunately, since the game theory was not particularly well-motivated, I did not find the overall story compelling. It is perhaps interesting that one can make deep learning learn to cooperate, but one could have illustrated the game theory equally well with other techniques.\n\nIn contrast, the paper \"Coco-Q: Learning in Stochastic Games with Side Payments\" by Sodomka et. al. is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning. I would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting.\n\nIt should also be noted that I was asked to review another ICLR submission entitled \"CONSEQUENTIALIST CONDITIONAL COOPERATION IN\nSOCIAL DILEMMAS WITH IMPERFECT INFORMATION\n\" which amazingly introduced the same \"Pong Player\u2019s Dilemma\" game as in this paper. \n\nNotice the following suspiciously similar paragraphs from the two papers:\n\nFrom \"MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING\":\nWe also look at an environment where strategies must be learned from raw pixels. We use the method\nof Tampuu et al. (2017) to alter the reward structure of Atari Pong so that whenever an agent scores a\npoint they receive a reward of 1 and the other player receives \u22122. We refer to this game as the Pong\nPlayer\u2019s Dilemma (PPD). In the PPD the only (jointly) winning move is not to play. However, a fully\ncooperative agent can be exploited by a defector.\n\nFrom \"CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION\":\nTo demonstrate this we follow the method of Tampuu et al. (2017) to construct a version of Atari Pong \nwhich makes the game into a social dilemma. In what we call the Pong Player\u2019s Dilemma (PPD) when an agent \nscores they gain a reward of 1 but the partner receives a reward of \u22122. Thus, in the PPD the only (jointly) winning\nmove is not to play, but selfish agents are again tempted to defect and try to score points even though\nthis decreases total social reward. We see that CCC is a successful, robust, and simple strategy in this\ngame.",
    "all_annotations": [
      {
        "review_id": "B1_TQ-clG",
        "annotator": "anno0",
        "importance": 1,
        "originality": 1,
        "method": 1,
        "presentation": 1,
        "interpretation": 0,
        "reproducibility": 0,
        "metareview": "maybe",
        "overall": 3,
        "evidence": 4,
        "constructiveness": 3
      }
    ],
    "conference": "iclr18",
    "review_id": "B1_TQ-clG",
    "gold_annotation": {
      "review_id": "B1_TQ-clG",
      "annotator": "anno0",
      "importance": 1,
      "originality": 1,
      "method": 1,
      "presentation": 1,
      "interpretation": 0,
      "reproducibility": 0,
      "metareview": "maybe",
      "overall": 3,
      "evidence": 4,
      "constructiveness": 3
    }
  },
  {
    "score": 3,
    "review_text": "The authors propose a scheme to generate questions based on some answer sentences, topics and question types. Topics are extracted from questions using similar words in question-answer pairs. It is similar to what we find in some Q&A systems (like lexical answer types in Watson). A sequence classifier is also used to tag the presence of topic words. Question types correspond mostly to salient questions words. LSTMs are used to encode the various inputs and generate the questions. \n\nThe paper is well written and easy to follow. I would expect more explanations why sentence classification and labeling results presented in Table 2 are so low. \n\nExperimental results on question generation are convincing and clearly indicate that the approach is effective to generate relevant and well-structured short questions. \n\nThe main weakness of the paper is the selected set of question types that seems to be a fuzzy combination of answer types and question types (for ex. yes/no). Some questions type can be highly ambiguous; for instance \u201cWhat\u201d might lead to a definition, a quantity, some named entities... Hence I suggest you revise your qt set. \n\nI would also suggest, for your next experiments, that you try to generate questions leading to answers with list of values. ",
    "all_annotations": [
      {
        "review_id": "B1chwjFlz",
        "annotator": "anno0",
        "importance": 0,
        "originality": 0,
        "method": 1,
        "presentation": 0,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "maybe",
        "overall": 3,
        "evidence": 4,
        "constructiveness": 4
      }
    ],
    "conference": "iclr18",
    "review_id": "B1chwjFlz",
    "gold_annotation": {
      "review_id": "B1chwjFlz",
      "annotator": "anno0",
      "importance": 0,
      "originality": 0,
      "method": 1,
      "presentation": 0,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "maybe",
      "overall": 3,
      "evidence": 4,
      "constructiveness": 4
    }
  },
  {
    "score": 3,
    "review_text": "The paper is not anonymized. In page 2, the first line, the authors revealed [15] is a self-citation and [15] is not anonumized in the reference list.\n\n",
    "tokenized_review_text": [
      "The paper is not anonymized.",
      "In page 2, the first line, the authors reveald [15] is a self-citation and [15] is not anonumized in the reference list."
    ],
    "all_annotations": [
      {
        "review_id": "B1fZIQcxM",
        "annotator": "anno0",
        "importance": 0,
        "originality": 0,
        "method": 0,
        "presentation": 0,
        "interpretation": 0,
        "reproducibility": 0,
        "metareview": "no",
        "overall": 1,
        "evidence": 3,
        "constructiveness": 1
      }
    ],
    "conference": "iclr18",
    "review_id": "B1fZIQcxM",
    "gold_annotation": {
      "review_id": "B1fZIQcxM",
      "annotator": "anno0",
      "importance": 0,
      "originality": 0,
      "method": 0,
      "presentation": 0,
      "interpretation": 0,
      "reproducibility": 0,
      "metareview": "no",
      "overall": 1,
      "evidence": 3,
      "constructiveness": 1
    }
  },
  {
    "score": 3,
    "review_text": "The article \"Do GANs Learn the Distribution? Some Theory and Empirics\" considers the important problem of quantifying whether the distributions obtained from generative adversarial networks come close to the actual distribution of images. The authors argue that GANs in fact generate the distributions with fairly low support.\n\nThe proposed approach relies on so-called birthday paradox which allows to estimate the number of objects in the support by counting number of matching (or very similar) pairs in the generated sample. This test is expected to experimentally support the previous theoretical analysis by Arora et al. (2017). The further theoretical analysis is also performed showing that for encoder-decoder GAN architectures the distributions with low support can be very close to the optimum of the specific (BiGAN) objective.\n\nThe experimental part of the paper considers the CelebA and CIFAR-10 datasets. We definitely see many very similar images in fairly small sample generated. So, the general claim is supported. However, if you look closely at some pictures, you can see that they are very different though reported as similar. For example, some deer or truck pictures. That's why I would recommend to reevaluate the results visually, which may lead to some change in the number of near duplicates and consequently the final support estimates.\n\nTo sum up, I think that the general idea looks very natural and the results are supportive. On theoretical side, the results seem fair (though I didn't check the proofs) and, being partly based on the previous results of Arora et al. (2017), clearly make a step further.",
    "all_annotations": [
      {
        "review_id": "B1g5pBTxz",
        "annotator": "anno0",
        "importance": 1,
        "originality": 1,
        "method": 1,
        "presentation": 0,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "maybe",
        "overall": 3,
        "evidence": 2,
        "constructiveness": 3
      }
    ],
    "conference": "iclr18",
    "review_id": "B1g5pBTxz",
    "gold_annotation": {
      "review_id": "B1g5pBTxz",
      "annotator": "anno0",
      "importance": 1,
      "originality": 1,
      "method": 1,
      "presentation": 0,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "maybe",
      "overall": 3,
      "evidence": 2,
      "constructiveness": 3
    }
  },
  {
    "score": 3,
    "review_text": "I have read authors' reply.  In response to authors' comprehensive reply and feedback. I upgrade my score to 6.\n\n-----------------------------\n\nThis paper presents a novel approach to calibrate classifiers for out of distribution samples. In additional to the original cross entropy loss, the \u201cconfidence loss\u201d  was proposed to guarantee the out of distribution points have low confidence in the classifier. As out of distribution samples are hard to obtain, authors also propose to use GAN generating \u201cboundary\u201d samples as out of distribution samples. \n\nThe problem setting is new and objective (1) is interesting and reasonable. However, I am not very convinced that objective (3) will generate boundary samples. Suppose that theta is set appropriately so that p_theta (y|x) gives a uniform distribution over labels for out of distribution samples. Because of the construction of U(y), which uniformly assign labels to generated out of distribution samples, the conditional probability p_g (y|x) should always be uniform so p_g (y|x) divided by p_theta (y|x) is almost always 1. The KL divergence in (a) of (3) should always be approximately 0 no matter what samples are generated. \n\nI also have a few other concerns: \n1. There seems to be a related work: \n[1] Perello-Nieto et al., Background Check: A general technique to build more reliable and versatile classifiers, ICDM 2016, \nWhere authors constructed a classifier, which output K+1 labels and the K+1-th label is the \u201cbackground noise\u201d label for this classification problem. Is the method in [1] applicable to this paper\u2019s setting?  Moreover, [1] did not seem to generate any out of distribution samples. \n\n2. I am not so sure that how the actual out of distribution detection was done (did I miss something here?). Authors repeatedly mentioned \u201cmaximum prediction values\u201d, but it was not defined throughout the paper. \nAlgorithm 1. is called \u201cminimization for detection and generating out of distribution (samples)\u201d, but this is only gradient descent, right? I do not see a detection procedure. Given the title also contains \u201cdetecting\u201d, I feel authors should write explicitly how the detection is done in the main body. \n",
    "all_annotations": [
      {
        "review_id": "B1ja8-9lf",
        "annotator": "anno0",
        "importance": 0,
        "originality": 1,
        "method": 1,
        "presentation": 0,
        "interpretation": 0,
        "reproducibility": 0,
        "metareview": "maybe",
        "overall": 4,
        "evidence": 4,
        "constructiveness": 4
      }
    ],
    "conference": "iclr18",
    "review_id": "B1ja8-9lf",
    "gold_annotation": {
      "review_id": "B1ja8-9lf",
      "annotator": "anno0",
      "importance": 0,
      "originality": 1,
      "method": 1,
      "presentation": 0,
      "interpretation": 0,
      "reproducibility": 0,
      "metareview": "maybe",
      "overall": 4,
      "evidence": 4,
      "constructiveness": 4
    }
  },
  {
    "score": 3,
    "review_text": "Quality\n\nThis paper demonstrates that convolutional and relational neural networks fail to solve visual relation problems by training networks on artificially generated visual relation data. This points at important limitations of current neural network architectures where architectures depend mainly on rote memorization.\n\nClarity\n\nThe rationale in the paper is straightforward. I do think that breakdown of networks by testing on increasing image variability is expected given that there is no reason that networks should generalize well to parts of input space that were never encountered before.\n\nOriginality\n\nWhile others have pointed out limitations before, this paper considers relational networks for the first time.\n\nSignificance \n\nThis work demonstrates failures of relational networks on relational tasks, which is an important message. At the same time, no new architectures are presented to address these limitations.\n\nPros\n\nImportant message about network limitations.\n\nCons\n\nStraightforward testing of network performance on specific visual relation tasks. No new theory development. Conclusions drawn by testing on out of sample data may not be completely valid.",
    "all_annotations": [
      {
        "review_id": "B1pcOYBlG",
        "annotator": "anno0",
        "importance": 1,
        "originality": 1,
        "method": 1,
        "presentation": 0,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "maybe",
        "overall": 3,
        "evidence": 1,
        "constructiveness": 3
      }
    ],
    "conference": "iclr18",
    "review_id": "B1pcOYBlG",
    "gold_annotation": {
      "review_id": "B1pcOYBlG",
      "annotator": "anno0",
      "importance": 1,
      "originality": 1,
      "method": 1,
      "presentation": 0,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "maybe",
      "overall": 3,
      "evidence": 1,
      "constructiveness": 3
    }
  },
  {
    "score": 3,
    "review_text": "Summary:\nThis paper proposes a simple recipe to preserve proximity to zero mean for activations in deep neural networks. The proposal is to replace the non-linearity in half of the units in each layer with its \"bipolar\" version -- one that is obtained by flipping the function on both axes.\nThe technique is tested on deep stacks of recurrent layers, and on convolutional networks with depth of 28, showing that improved results over the baseline networks are obtained. \n\nClarity:\nThe paper is easy to read. The plots in Fig. 2 and the appendix are quite helpful in improving presentation. The experimental setups are explained in detail. \n\nQuality and significance:\nThe main idea from this paper is simple and intuitive. However, the experiments to support the idea do not seem to match the motivation of the paper. As stated in the beginning of the paper, the motivation behind having close to zero mean activations is that this is expected to speed up training using gradient descent. However, the presented results focus on the performance on held-out data instead of improvements in training speed. This is especially the case for the RNN experiments.\n\nFor the CIFAR-10 experiment, the training loss curves do show faster initial progress in learning. However, it is unclear that overall training time can be reduced with the help of this technique. To evaluate this speed up effect, the dependence on the choice of learning rate and other hyperparameters should also be considered.\n\nNevertheless, it is interesting to note the result that the proposed approach converts a deep network that does not train into one which does in many cases. The method appears to improve the training for moderately deep convolutional networks without batch normalization (although this is tested on a single dataset), but is not practically useful yet since the regularization benefits of Batch Normalization are also taken away.\n",
    "all_annotations": [
      {
        "review_id": "B1qbgHcxz",
        "annotator": "anno0",
        "importance": 1,
        "originality": 0,
        "method": 1,
        "presentation": 1,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "maybe",
        "overall": 3,
        "evidence": 3,
        "constructiveness": 3
      }
    ],
    "conference": "iclr18",
    "review_id": "B1qbgHcxz",
    "gold_annotation": {
      "review_id": "B1qbgHcxz",
      "annotator": "anno0",
      "importance": 1,
      "originality": 0,
      "method": 1,
      "presentation": 1,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "maybe",
      "overall": 3,
      "evidence": 3,
      "constructiveness": 3
    }
  },
  {
    "score": 3,
    "review_text": "The paper investigates the iterative estimation view on gated recurrent networks (GNN). Authors observe that the average estimation error between a given hidden state and the last hidden state  gradually decreases toward zeros. This suggest that GNN are bias toward an identity mapping and learn to preserve the activation through time.\nGiven this observation, authors then propose RIN, a new RNN parametrization where the hidden to hidden matrix is decomposed as a learnable weight matrix plus the identity matrix.\nAuthors evaluate their RIN on the adding, sequential MNIST and the baby tasks and show that their IRNN outperforms the IRNN and LSTM models.\n\nQuestions:\n- Section 2 suggests that use of the gate  in GNNs encourages to learn an identity mapping. Does the average iteration error behaves differently in case of a tanh-RNN ?\n- It seems from Figure 4 (a) that the average estimation error is higher for RIN than IRNN and LSTM and only decrease toward zero at the very end. What could explain this phenomenon?\n- While the LSTM baseline matches the results of Le et al., later work such as Recurrent Batch Normalization or Unitary Evolution RNN have demonstrated much better performance with a vanilla LSTM on those tasks (outperforming both IRNN and RIN). What could explain this difference in the performances?\n- Unless I am mistaken, Gated Orthogonal Recurrent Units: On Learning to Forget from Jing et al. also reports better performances for the LSTM (and GRU) baselines that outperform RIN on the baby tasks with mean performances of 58.2 and 56.0 for GRU and LSTM respectively?\n\n- Quality/Clarity:\nThe paper is well written and pleasant to read\n\n- Originality:\nLooking at RNN from an iterative refinement point of view seems novel.\n\n- Significance:\nWhile looking at RNN from an iterative estimation is interesting, the experimental part does not really show what are the advantages of the propose RIN. In particular, the LSTM baseline seems to weak compared to other works.",
    "all_annotations": [
      {
        "review_id": "B1qhp-qeG",
        "annotator": "anno0",
        "importance": 1,
        "originality": 1,
        "method": 0,
        "presentation": 1,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "no",
        "overall": 3,
        "evidence": 3,
        "constructiveness": 3
      }
    ],
    "conference": "iclr18",
    "review_id": "B1qhp-qeG",
    "gold_annotation": {
      "review_id": "B1qhp-qeG",
      "annotator": "anno0",
      "importance": 1,
      "originality": 1,
      "method": 0,
      "presentation": 1,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "no",
      "overall": 3,
      "evidence": 3,
      "constructiveness": 3
    }
  },
  {
    "score": 3,
    "review_text": "This submission proposes a new seq2sel solution by adopting two new techniques, a sequence-to-set model and column attention mechanism. They show performance improve over existing studies on WikiSQL dataset.\n\nWhile the paper is written clearly, the contributions of the work heavily depends on the WikiSQL dataset. It is not sure if the approach is generally applicable to other sequence-to-sql workloads. Detailed comments are listed below:\n\n1. WikiSQL dataset contains only a small class of SQL queries, with aggregation over single table and various filtering conditions. It does not involve any complex operator in relational database system, e.g., join and groupby. Due to its simple structure, the problem of sequence-to-sql translation over WikiSQL is actually simplified as a parameter selection problem for a fixed template. This greatly limits the generalization of approaches only applicable to WikiSQL. The authors are encouraged to explore other datasets available in the literature.\n\n2. The \"order-matters\" motivation is not very convincing. It is straightforward to employ a global ordering approach to rank the columns and filtering conditions based on certain rules, e.g., alphabetical order. That could ensure the orders in the SQL results are always consistent.\n\n3. The experiments do not fully verify how the approaches bring performance improvements. In the current version, the authors only report superficial accuracy results on final outcomes, without any deep investigation into why and how their approach works. For instance, they could verify how much accuracy improvement is due to the insensitivity to order in filtering expressions.\n\n4. They do not compare against state-of-the-art solution on column and expression selection. While their attention mechanism over the columns could bring performance improvement, they should have included experiments over existing solutions designed for similar purpose. In (Yin, et al., IJCAI 2016), for example, representations over the columns are learned to generate better column selection.\n\nAs a conclusion, I find the submission contains certain interesting ideas but lacks serious research investigations. The quality of the paper could be much enhanced, if the authors deepen their studies on this direction.",
    "all_annotations": [
      {
        "review_id": "B1y7_3YgM",
        "annotator": "anno0",
        "importance": 0,
        "originality": 0,
        "method": 1,
        "presentation": 1,
        "interpretation": 1,
        "reproducibility": 0,
        "metareview": "no",
        "overall": 4,
        "evidence": 4,
        "constructiveness": 4
      }
    ],
    "conference": "iclr18",
    "review_id": "B1y7_3YgM",
    "gold_annotation": {
      "review_id": "B1y7_3YgM",
      "annotator": "anno0",
      "importance": 0,
      "originality": 0,
      "method": 1,
      "presentation": 1,
      "interpretation": 1,
      "reproducibility": 0,
      "metareview": "no",
      "overall": 4,
      "evidence": 4,
      "constructiveness": 4
    }
  }
]
